# AI-RAG FLOW -Local AI Powered Datawharehouse System-
ğŸŒ[DEMO](https://ai-ragflow.com/)
## ğŸ“– Table of Contents
- [ğŸ“ Introduction](#introduction)
- [ğŸ“Š Features](#features)
- [ğŸ“‚ Project Structure](#project-structure)
- [ğŸ”§ Setup Instructions](#setup-instructions)
  - [Clone the Repository](#clone-the-repository)
  - [Install Dependencies](#install-dependencies)
  - [Configure Environment Variables](#configure-environment-variables)
- [âœ… Usage](#usage)
- [âš™ï¸ Customization](#customization)
  - [Change the Embedding Model](#change-the-embedding-model)
- [ğŸ“œ License](#license)
- [ğŸ¤ Contributing](#contributing)
- [ğŸ“¬ Contact](#contact)


## ğŸ“ Introduction
### ğŸš€ Motivation

Currently, we rely on ChatGPT for searching, but it presents several issues:

#### âš ï¸ Problems:
- ğŸ” **Hallucination** â€“ The model sometimes generates inaccurate or misleading information.
- ğŸ“‰ **Incomplete Results** â€“ It does not retrieve all relevant matches.

#### âœ… Solution:
This project is a **Retrieval-Augmented Generation (RAG) system** designed to handle large datasets, specifically for **legal text analysis and retrieval**. It leverages:
- ğŸ—„ï¸ **MongoDB** for document storage and retrieval.
- ğŸ§  **OpenAI embeddings** for semantic search.
- âš¡ **Annoy-based vector search** for efficient similarity-based matching.

### ğŸ”¥ Key Improvements:
- ğŸ›¡ï¸ **Reduced Hallucination** â€“ By restricting searches to a **local domain**, we drastically lower the chances of misinformation.
- ğŸ¯ **Higher Accuracy** â€“ Utilizing the **latest OpenAI embedding model**, we aim to surpass existing solutions in precision and relevance.
- ğŸ’° **Optimized API Usage** â€“ By caching and saving search queries, we **reduce API costs** while maintaining efficient retrieval.


### ğŸ“‚ Dataset Used
For testing, this system utilizes the Open Australian Legal Corpus, available on Kaggle:ğŸ”— [Open Australian Legal Corpus](https://www.kaggle.com/datasets/umarbutler/open-australian-legal-corpus)
## ğŸ“Š Features
### ğŸ”¹ Key Features
- âœ… Scalable Large-Scale Data Processing â€“ Designed for handling extensive datasets efficiently.
- ğŸ”¥ ChatGPT API with the Latest Embedding Model â€“ Ensures high-accuracy text search and retrieval.
- âš¡ Optimized RAG Pipeline â€“ Balances retrieval accuracy and generation quality for better responses.
- ğŸ’¾ Local Vector Database Storage â€“ Saves embeddings locally to minimize API calls, reducing cost and improving speed.
- ğŸ— Customizable & Extendable â€“ Easily tweak settings, vector database configurations, and embeddings.
### ğŸ’¡ How It Works
- Preprocess Large Datasets : Loads and processes structured or unstructured data.
- Generate & Store Embeddings : Uses ChatGPT embeddings and saves them locally for vector-based search.
- Efficient Retrieval & Augmentation : Quickly fetches relevant data using an optimized vector search engine.
- ChatGPT-Powered Responses : The system refines retrieved results and generates intelligent, context-aware responses.
- API Optimization : By caching embeddings and search results, the system reduces redundant API calls.

### ğŸ”¹ Data Processing & Storage
- âœ… Data Ingestion â€“ Reads and loads structured/unstructured documents from a JSONL file into MongoDB.
- âœ… Embedding Generation â€“ Uses OpenAI's embedding API to convert text into high-dimensional vector representations for efficient search.

### ğŸ” Intelligent Search & Retrieval
- âš¡ Vector Search â€“ Employs Annoy (Approximate Nearest Neighbors) for fast and scalable vector-based document retrieval.
- ğŸ“Š Similarity Ranking â€“ Uses cosine similarity to rank and filter the most relevant documents based on the user query.

### ğŸ§  AI-Powered Summarization
ğŸ’¡ Context-Aware Summarization â€“ Extracts insights from the top 10 retrieved cases and summarizes results using ChatGPT.

### âš™ï¸ Scalable & Modular Design
#### ğŸ— Modular Architecture:
- ğŸ“‚ Data Ingestion: Reads documents from a JSONL file and stores them in MongoDB.
- ğŸ§ Embedding Generation: Uses OpenAI's embedding API to convert text into high-dimensional vectors.
- âš¡ Vector Search:Utilizes Annoy for efficient, approximate nearest neighbor searches.
- ğŸ“Š Similarity Ranking:Applies cosine similarity to filter and rank relevant documents.
- ğŸ“ Summarization:Summarizes the top 10 retrieved cases using ChatGPT.


## ğŸ“‚ Project Structure

- **config.py**:Loads environment variables (.env) 
- **ingest.py(ingest_legal_Corpus)** :Ingests CSV data into MongoDB 
- **db.py**:ETL query, Embedding computation, Annoy index building, and similarity search
- **summarizer.py** : ChatGPT summarization of retrieved cases
- **main.py**:    Main entry point for user query processing
- **.env**:   Contains sensitive variables (API keys, MongoDB URI, etc.)
- **requirements.txt**:   Python dependencies

## ğŸ”§ Setup Instructions
### 1. ğŸ–¥ï¸ Clone the Repository

```bash
git clone https://github.com/yourusername/vector-rag-system.git
cd vector-rag-system
```
### 2. ğŸ“¦ Install Dependencies

Ensure you have Python installed, then run:
```bash
pip install -r requirements.txt
```
If you do not have MongoDB:
ğŸ”— [Install MongoDB](https://www.mongodb.com/docs/manual/installation/)
### 3. ğŸ”§ Configure Environment Variables

Create a .env file in the project root with the following content:

```bash
OPENAI_API_KEY=your_openai_api_key_here # Get OpenAI'API 
MONGO_URI=mongodb://localhost:27017/   # Local 
JSONL_PATH=your_data.jsonl             # Add the path for the ingesting
EMBEDDING_MODEL=text-embedding-3-large # The latest model
EMBEDDING_DIMENSIONS=1024              # This is the configuration of text-embedding-3-large
```
ğŸ”— [OpenAI'API](https://openai.com/api/)
## âœ… Usage

### 1. ğŸ”„ Prepare Data
- Each Parsing needs to be implmented beforehand, then load your JSONL data into MongoDB by running:
```bash
python ingest.py
```
### 2. âš¡ Process Queries
- Launch the main application to handle user queries:
```bash
python main.py
```
- When prompted, enter your query. The system will:
Compute the embedding for your query.
- Search the Annoy index for similar documents.
- Summarize the top 10 matching cases using ChatGPT.
- Display the summarized insights in the console.

## âš™ï¸ Customization
- Embedding Model: Change the EMBEDDING_MODEL in your .env file to use a different OpenAI model if needed.
- MongoDB Configuration: Adjust the MONGO_URI in your .env file to connect to a different MongoDB instance.
- Annoy Settings: Tweak parameters such as VECTOR_SIZE and ANNOY_TREE_COUNT in vector_db.py to suit your data and - performance requirements.
- Summarization Prompt: Modify the prompt in summarizer.py to tailor the summarization output.
## ğŸš€ Future Improvements
- Add More DataSet
- Advanced Vector DBs: Experiment with specialized vector databases like Pinecone, Milvus, or Qdrant for even faster search performance.
## ğŸ“œ License
#### âš ï¸ This project is licensed under the MIT License.

## ğŸ¤Contributer
### ğŸ‘¤Software Engineer: Yuto Mori (Auther)
### ğŸ‘¤Designer: Ambre Grimault 

## ğŸ“¬Contact
Contributions, issues, and feature requests are welcome! Please check the issues page for known issues and to submit new ones.
#### ğŸ“§Email: contact.ragflow@gmail.com 
